likelihood <- function(h, n, p){
lh <- dbinom(h, n, p)
lh
}
#prior as a beta function
dbeta(p, 1, 1)
# Set the numer of tosses.
n <- 100
# Set the number of heads obtained.
h <- 73
"Now, the acceptance probability (R, see equations in Step 3) will
be the minimum value: 1 or the ratio of posterior
probabilities given the different p. We express this equation in R language as follows"
R <- likelihood(h,n,p_prime)/likelihood(h,n,p) * (dbeta(p_prime,1,1)/dbeta(p,1,1))
# Set the lenght of the loop (Marcov Chain, number of iterations).
nrep <- 5000
# Step 1) Set an initial value for p (prior)
p <- runif(1, 0, 1)  #(random value from 0 to 1)
#Step 2) Propose a new value of p, called p-prime.
p_prime <- p + runif(1, -0.05, 0.05)
"The advantage of this method is that we avoid to compute the marginal likelihood,
that is often difficult to obtain with more complex models.
Letâ€™s stop here a little bit to explain each term of this equation."
#Likehood as a binomial function
likelihood <- function(h, n, p){
lh <- dbinom(h, n, p)
lh
}
#prior as a beta function
dbeta(p, 1, 1)
# Set the numer of tosses.
n <- 100
# Set the number of heads obtained.
h <- 73
"Now, the acceptance probability (R, see equations in Step 3) will
be the minimum value: 1 or the ratio of posterior
probabilities given the different p. We express this equation in R language as follows"
R <- likelihood(h,n,p_prime)/likelihood(h,n,p) * (dbeta(p_prime,1,1)/dbeta(p,1,1))
# Set the lenght of the loop (Marcov Chain, number of iterations).
nrep <- 5000
# Start the loop (MCMC)
for (i in 1:nrep) {
# Obtain a new proposal value for p
p_prime <- p + runif(1, -0.05,0.05)
# Avoid values out of the range 0 - 1
if (p_prime < 0) {p_prime <- abs(p_prime)}
if (p_prime > 1) {p_prime <- 2 - p_prime}
# Compute the acceptance proability using our likelihood function and the
# beta(1,1) distribution as our prior probability.
R <- likelihood(h,n,p_prime)/likelihood(h,n,p) * (dbeta(p_prime,1,1)/dbeta(p,1,1))
# Accept or reject the new value of p
if (R > 1) {R <- 1}
random <- runif (1,0,1)
if (random < R) {
p <- p_prime
}
# Store the likelihood of the accepted p and its value
posterior[i,1] <- log(likelihood(h, n, p))
posterior[i,2] <- p
print(i)
}
posterior <- data.frame()
# Set the lenght of the loop (Marcov Chain, number of iterations).
nrep <- 5000
# Start the loop (MCMC)
for (i in 1:nrep) {
# Obtain a new proposal value for p
p_prime <- p + runif(1, -0.05,0.05)
# Avoid values out of the range 0 - 1
if (p_prime < 0) {p_prime <- abs(p_prime)}
if (p_prime > 1) {p_prime <- 2 - p_prime}
# Compute the acceptance proability using our likelihood function and the
# beta(1,1) distribution as our prior probability.
R <- likelihood(h,n,p_prime)/likelihood(h,n,p) * (dbeta(p_prime,1,1)/dbeta(p,1,1))
# Accept or reject the new value of p
if (R > 1) {R <- 1}
random <- runif (1,0,1)
if (random < R) {
p <- p_prime
}
# Store the likelihood of the accepted p and its value
posterior[i,1] <- log(likelihood(h, n, p))
posterior[i,2] <- p
print(i)
}
par(mfrow= c(1,2))
prior <- rbeta(5000, 1,1)
plot(1:5000 ,posterior$V2, cex=0, xlab = "generations", ylab = "p",
main = "trace of MCMC\n accepted values of parameter p\n prior = beta(1,1) generations = 5000")
lines(1:5000, posterior$V2, cex=0)
abline(h=mean(posterior$V2), col="red")
plot(density(posterior$V2), xlim = c(min(min(prior),min((posterior$V2))), max(max(prior),max((posterior$V2)))),
ylim = c(0, max(max(density(prior)$y),max((density(posterior$V2)$y)))), main= "prior VS posterior\n prior= beta(1,1)",
lwd=3, col="red")
lines(density(prior), lwd=3, lty=2, col="blue")
legend("topleft", legend=c("prior density","posterior density"),
col=c("blue","red"), lty=c(3,1), lwd=c(3,3), cex = 1)
library(dplyr)
library(ltm)
library(psych)
library(mirt)
source("C://Users//Christian//Documents//GitHub//CausalModel_FaultUnderstanding//load_consent_create_indexes_E1.R")
"Remove participants for whom we did not take the qualification test"
df <- df_E1[complete.cases(df_E1[,"qualification_score"]),] #left with 3699 rows
"Replace false for 0(zero) and true for one(1)"
df$test1_ <-  ifelse(df$test1=="true",1,0)
df$test2_ <-  ifelse(df$test2=="true",1,0)
df$test3_ <-  ifelse(df$test3=="true",1,0)
df$test4_ <-  ifelse(df$test4=="true",1,0)
df <- df %>% dplyr::select(test1_,test2_,test3_,test4_)
write.csv(df,"C://Users//Christian//Documents//GitHub//CausalModel_FaultUnderstanding//E1_QualificationTestResults.csv")
IRT_model_2PL <- ltm(df ~ z1, IRT.param=TRUE)
IRT_model_2PL
plot(IRT_model_2PL, type="ICC")
plot(IRT_model_2PL, type="IIC", items=0)
factor.scores.ltm(IRT_model_2PL)
df
View (df)
?ltm
df_factors <- factor.scores.ltm(IRT_model_2PL)
summary(df_factors)
type(df_factors)
class(df_factors)
dim(df_factors)
length(df_factors)
df_factors$score.dat
df_score.dat <-  data.frame(df_factors$score.dat)
head(df_score.dat)
?left_join
df_new <- left_join(df,df_new,by=c("test1_"="test1_","test2_"="test2_","test3_"="test3_","test4_"="test4_")))
df_new <- left_join(df,df_new,by=c("test1_"="test1_","test2_"="test2_","test3_"="test3_","test4_"="test4_"))
left_join(df,df_new,by=c("test1_"="test1_","test2_"="test2_","test3_"="test3_","test4_"="test4_"))
df_new <- left_join(df,df_score.dat,by=c("test1_"="test1_","test2_"="test2_","test3_"="test3_","test4_"="test4_"))
View(df_new)
library(dplyr)
library(ltm)
library(psych)
library(mirt)
source("C://Users//Christian//Documents//GitHub//CausalModel_FaultUnderstanding//load_consent_create_indexes_E1.R")
"Remove participants for whom we did not take the qualification test"
df <- df_E1[complete.cases(df_E1[,"qualification_score"]),] #left with 3699 rows
"Replace false for 0(zero) and true for one(1)"
df$test1_ <-  ifelse(df$test1=="true",1,0)
df$test2_ <-  ifelse(df$test2=="true",1,0)
df$test3_ <-  ifelse(df$test3=="true",1,0)
df$test4_ <-  ifelse(df$test4=="true",1,0)
df_tests <- df %>% dplyr::select(test1_,test2_,test3_,test4_)
write.csv(df_tests,"C://Users//Christian//Documents//GitHub//CausalModel_FaultUnderstanding//E1_QualificationTestResults.csv")
IRT_model_2PL <- ltm(df_tests ~ z1, IRT.param=TRUE)
IRT_model_2PL
plot(IRT_model_2PL, type="ICC")
plot(IRT_model_2PL, type="ICC", items=c(1,3))
plot(IRT_model_2PL, type="IIC", items=0)
factor.scores.ltm(IRT_model_2PL)
#LEFT JOIN to associate the new difficulty scores (z1) to the partipants.
df_new <- left_join(df,df_score.dat,by=c("test1_"="test1_","test2_"="test2_","test3_"="test3_","test4_"="test4_"))
factors <- factor.scores.ltm(IRT_model_2PL)
df_score.dat <- data.frame(factors$score.dat)
factors
#LEFT JOIN to associate the new difficulty scores (z1) to the partipants.
df_new <- left_join(df,df_score.dat,by=c("test1_"="test1_","test2_"="test2_","test3_"="test3_","test4_"="test4_"))
#Store in the original file the new difficulty scores (z1) of the partipants
write.csv(df_new,"C://Users//Christian//Documents//GitHub//CausalModel_FaultUnderstanding//E1_QualificationTest_IRT.csv")
View(df_new)
plot(df_new$z1)
hist(df_new$z1)
plot(df_new$years_programming, df_new$z1)
lm1 <- lm(z1 ~ years_programming, df_new)
lm1
lm1 <- lm(z1 ~ years_programming + I(years_programming^2), df_new)
lm1
plot(lm1)
plot(df_new$years_programming, df_new$qualification_score)
#Visualizing the results
plot(df_new$years_programming, df_new$z1)
year <- seq(from=0, to=40, by=0.1)
effect_plot(lm1, pred = year, interval = TRUE)
library(ggplot2)
effect_plot(lm1, pred = year, interval = TRUE)
effect_plot
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
effect_plot
plot(lm1,pred=year)
